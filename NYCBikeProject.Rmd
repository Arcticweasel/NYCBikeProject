---
title: "NYCBikeProject"
author: "Laura, Felix"
date: "7 4 2020"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Load Packages

```{r include=FALSE}
library(tidyverse)
library(lubridate)
library(imputeTS)
library(timeDate)
library(fastDummies)
source("Routinen/ANNUtils.r")
```


# NYC bike project

## Überblick

In diesem Anwendungsprojekt werden Daten der City of New York bearbeitet, visualisiert und analysiert, die Informationen zum städtischen Bikesharing aus dem Jahr 2016 enthalten. Das Hauptaugenmerk in diesem Projekt liegt darauf, möglichst sinnvoll und genau die Anzahl der Nutzer an den verschiedenen Stationen vorherzusagen. Zu diesem Zweck werden zusätzlich Wetterdaten der Stadt New York hinzugezogen.

## Daten

Die Daten zum Bikesharing sind folgendermaßen strukturiert. Jede Zeile des ursprünglichen Datensatzes repräsentiert eine Fahrt mit einem Fahrrad der Bikesharing Flotte. Zu jeder Fahrt ist die Dauer der Leihe, der Start- und Endzeitpunkt, sowie die Start- und Endstation festgehalten. Außerdem ist die Indentifikationsnummer des jeweiligen Fahrrades vorhanden. Des Weiteren sind Kundeninformationen festgehalten. Das Geschlecht und Geburtsjahr, sowie der Nutzertyp. Hier wird zwischen Customer und Subscriber unterschieden. Subscriber haben ein jährliches Abonnement abgeschlossen, wohingegen Customer einen Ein- oder Drei-Tages-Pass erworben haben.

Die Wetterdaten halten die täglichen Maximal- und Minimaltemperaturen fest, sowie die Tagesdurchschnittstemperatur. Des Weiteren geben Sie Auskunft über den gefallenen Regen, sowie den neuen Schneefall und die Tiefe der aktuellen Schneedecke.

## Forschungsfragen

Nach einigen Diskussionen und Analysen der Daten haben sich zwei interessante Forschungsfragen ergeben.

1. Können die Nutzerzahlen pro Station und Stunde zuverlässig vorrausgesagt werden?   und

2. Können die Nutzerzahlen pro Tag in Abhängigkeit zum Wetter vorrausgesagt werden?

## Daten Laden

Zunächst werden die Datensätze geladen, die die Wetterdaten enthalten. Je Forschungsfrage gibt es einen eigenen Datensatz. Der Datensatz weather enthält tägliche Wetterdaten, wohingegen der Datensatz hourly_weather stündliche Wetterdaten enthält.

```{r, eval=FALSE, echo=TRUE}
weather <- read.csv("Data/weather_data_nyc_centralpark_2016(1).csv")
hourly_weather <- read.csv("Data/hourly_weather.csv")
```


Da die Bikesharingdaten von der Stadt New York monatlich gespeichert werden, werden sie zunächst je Monat in einen Data Frame geladen.

```{r, eval=FALSE, echo=TRUE}

januar <- read_csv("H:/Projekt Daten/201608-citibike-tripdata/201601-citibike-tripdata.csv")
februar <- read_csv("H:/Projekt Daten/201608-citibike-tripdata/201602-citibike-tripdata.csv")
maerz <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201603-citibike-tripdata.csv")
april <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201604-citibike-tripdata.csv")
mai <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201605-citibike-tripdata.csv")
juni <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201606-citibike-tripdata.csv")
juli <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201607-citibike-tripdata.csv")
august <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201608-citibike-tripdata.csv")
september <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201609-citibike-tripdata.csv")
oktober <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201610-citibike-tripdata.csv")
november <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201611-citibike-tripdata.csv")
dezember <-read_csv("H:/Projekt Daten/201608-citibike-tripdata/201612-citibike-tripdata.csv")
```


## Preprocessing

Da es in R schwierig sein kann mit Spaltennamen zu arbeiten, die Leerzeichen enthalten, werden diese zunächst durch Unterstriche ersetzt.

```{r, eval=FALSE, echo=TRUE}
names(januar) <- str_replace_all(names(januar), c(" " = "_"))
names(februar) <- str_replace_all(names(februar), c(" " = "_"))
names(maerz) <- str_replace_all(names(maerz), c(" " = "_"))
names(april) <- str_replace_all(names(april), c(" " = "_"))
names(mai) <- str_replace_all(names(mai), c(" " = "_"))
names(juni) <- str_replace_all(names(juni), c(" " = "_"))
names(juli) <- str_replace_all(names(juli), c(" " = "_"))
names(august) <- str_replace_all(names(august), c(" " = "_"))
names(september) <- str_replace_all(names(september), c(" " = "_"))
names(oktober) <- str_replace_all(names(oktober), c(" " = "_"))
names(november) <- str_replace_all(names(november), c(" " = "_"))
names(dezember) <- str_replace_all(names(dezember), c(" " = "_"))
```

In einem nächsten Schritt werden die Spaltennamen der Monate Oktober bis Dezember an die Namen der anderen Monate angepasst, damit ein Datensatz entstehen kann, der alle Monate umfasst.

```{r, eval=FALSE, echo=TRUE}
dezember <- dezember %>%
  rename(tripduration = Trip_Duration,
         starttime = Start_Time,
         stoptime = Stop_Time,
         start_station_id = Start_Station_ID,
         start_station_name = Start_Station_Name,
         start_station_latitude = Start_Station_Latitude,
         start_station_longitude = Start_Station_Longitude,
         end_station_id = End_Station_ID,
         end_station_name = End_Station_Name,
         end_station_latitude = End_Station_Latitude,
         end_station_longitude = End_Station_Longitude,
         bikeid = Bike_ID,
         usertype = User_Type,
         birth_year = Birth_Year,
         gender = Gender)

november <- november %>%
  rename(tripduration = Trip_Duration,
         starttime = Start_Time,
         stoptime = Stop_Time,
         start_station_id = Start_Station_ID,
         start_station_name = Start_Station_Name,
         start_station_latitude = Start_Station_Latitude,
         start_station_longitude = Start_Station_Longitude,
         end_station_id = End_Station_ID,
         end_station_name = End_Station_Name,
         end_station_latitude = End_Station_Latitude,
         end_station_longitude = End_Station_Longitude,
         bikeid = Bike_ID,
         usertype = User_Type,
         birth_year = Birth_Year,
         gender = Gender)

oktober <- oktober %>%
  rename(tripduration = Trip_Duration,
         starttime = Start_Time,
         stoptime = Stop_Time,
         start_station_id = Start_Station_ID,
         start_station_name = Start_Station_Name,
         start_station_latitude = Start_Station_Latitude,
         start_station_longitude = Start_Station_Longitude,
         end_station_id = End_Station_ID,
         end_station_name = End_Station_Name,
         end_station_latitude = End_Station_Latitude,
         end_station_longitude = End_Station_Longitude,
         bikeid = Bike_ID,
         usertype = User_Type,
         birth_year = Birth_Year,
         gender = Gender)
```

Aufgrund der Datenmenge werden die Monate einzeln abgespeichert und nach dem Laden zu einem Data Frame zusammengefasst. So ist während der Projektarbeit sichergestellt, dass alle Teammitglieder zu jeder Zeit über GitHub die jeweils aktuellen Daten nutzen können und keine Diskrepanzen entstehen.

```{r, eval=FALSE, echo=TRUE}
saveRDS(januar, "Bike_Data_Jan.RDS")
saveRDS(februar, "Bike_Data_Feb.RDS")
saveRDS(maerz, "Bike_Data_Mrz.RDS")
saveRDS(april, "Bike_Data_Apr.RDS")
saveRDS(mai, "Bike_Data_Mai.RDS")
saveRDS(juni, "Bike_Data_Jun.RDS")
saveRDS(juli, "Bike_Data_Jul.RDS")
saveRDS(august, "Bike_Data_Aug.RDS")
saveRDS(september, "Bike_Data_Sep.RDS")
saveRDS(oktober, "Bike_Data_Okt.RDS")
saveRDS(november, "Bike_Data_Nov.RDS")
saveRDS(dezember, "Bike_Data_Dez.RDS")
```

Die einzelnen Datensätze für jeden Monat werden geladen und in einem Datensatz mit dem Namen "bike" zusammengefasst. Da in den Datensätzen Januar bis September die Start- und Stopzeiten als <character> formatiert sind, in den Monaten Oktober bis Dezember aber als Unixtime bzw. bereits richtig umgewandelt als <datetime>, muss vor der Zusammenführung das Format vereinheitlicht werden. Hierfür wird in den Monaten Januar bis September der Datentyp von <character> zu <datetime> umgewandelt. So ist weiteres Arbeiten mit den Datumsangaben problemlos möglich.

```{r}
df1 <- read_rds("Data/Bike_Data_Jan.RDS")
df1$starttime <- mdy_hms(df1$starttime)
df1$stoptime <- mdy_hms(df1$stoptime)

df2 <- read_rds("Data/Bike_Data_Feb.RDS")
df2$starttime <- mdy_hms(df2$starttime)
df2$stoptime <- mdy_hms(df2$stoptime)

df3 <- read_rds("Data/Bike_Data_Mrz.RDS")
df3$starttime <- mdy_hms(df3$starttime)
df3$stoptime <- mdy_hms(df3$stoptime)

df4 <- read_rds("Data/Bike_Data_Apr.RDS")
df4$starttime <- mdy_hms(df4$starttime)
df4$stoptime <- mdy_hms(df4$stoptime)

df5 <- read_rds("Data/Bike_Data_Mai.RDS")
df5$starttime <- mdy_hms(df5$starttime)
df5$stoptime <- mdy_hms(df5$stoptime)

df6 <- read_rds("Data/Bike_Data_Jun.RDS")
df6$starttime <- mdy_hms(df6$starttime)
df6$stoptime <- mdy_hms(df6$stoptime)

df7 <- read_rds("Data/Bike_Data_Jul.RDS")
df7$starttime <- mdy_hms(df7$starttime)
df7$stoptime <- mdy_hms(df7$stoptime)

df8 <- read_rds("Data/Bike_Data_Aug.RDS")
df8$starttime <- mdy_hms(df8$starttime)
df8$stoptime <- mdy_hms(df8$stoptime)

df9 <- read_rds("Data/Bike_Data_Sep.RDS")
df9$starttime <- mdy_hms(df9$starttime)
df9$stoptime <- mdy_hms(df9$stoptime)

df10 <- read_rds("Data/Bike_Data_Okt.RDS")
df11 <- read_rds("Data/Bike_Data_Nov.RDS")
df12 <- read_rds("Data/Bike_Data_Dez.RDS")


bike <- rbind(df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12)
rm(df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12)
```


### Missing Values

Im nächsten Schritt des Preprocessing wird im gesamten Datensatz nach fehlenden Werten gesucht und diese entfernt oder ersetzt. Dies ist notwendig, da die späteren Analyseergebnisse durch fehlende Werte beeinträchtigt werden können.

```{r, eval=FALSE, echo=TRUE}

# Überprüfe auf Missing values

apply(bike, 2, function(x) sum(is.na(x)))

apply(weather, 2, function(x) sum(is.na(x)))


# Mögliche Variante für missing Values beim Wetter 

weather_test <- weather
is.na(weather_test$precipitation) <- which(weather$precipitation == 'T')
is.na(weather_test$snow.fall) <- which(weather$snow.fall == 'T')
is.na(weather_test$snow.depth) <- which(weather$snow.depth == 'T')

weather_test$precipitation <- as.numeric(weather_test$precipitation)

weather_test[is.na(weather_test$precipitation),"precipitation"] <- 0.01

weather_test_interpolated <- na_interpolation(weather_test$precipitation, option = "spline")
```

Die Spalten usertype und birth_year enthalten als einzige fehlende Werte. Um einen sauberen Datensatz zu erhalten, haben wir uns dazu entschlossen die Zeilen, die fehlende Werte enthalten aus dem Datensatz zu entfernen. Die hat zwei Gründe. Zum Einen kann das Geburtsjahr, sowie auch der Nutzertyp (Subscriber oder Customer) nicht sinnvoll interpoliert werden. Es können von den davor und danach liegenden Datenpunkten keine sinnvollen Rückschlüsse auf die fehlenden Werte gezogen werden. Zum Anderen liegt hier ein ausreichend großer Datensatz vor (über 13 000 000 Zeilen), sodass der Informationsverlust, der durch das Löschen von ca. 1 600 000 Zeilen gering ist. So können Verzerrungen durch falsch interpolierte Werte vermieden werden.


```{r}
bike <- na.omit(bike)
```
```{r, eval=FALSE, echo=TRUE}

apply(bike, 2, function(x) sum(is.na(x)))

```


### Tidy Data

#### Tägliche Wetterdaten

Um mit den täglichen Wetterdaten arbeiten zu können, muss zunächst die Spalte date in das richtige Datumsformat überführt werden. Die Temperaturangaben in Fahrenheit werden in Celsius, die Menge des Regens und des Schnees in Inches in Millimeter umgerechnet. Kann die Menge des gefallenen Regens oder Schnees nicht gemessen werden, wird dies durch ein T gekennzeichnet. Um mit den Daten problemlos arbeiten zu können, wird das T durch den Wert 0.01 ersetzt. So ist ein numerischer Wert vorhanden, der kennzeichnet, dass Regen oder Schnee gefallen ist. Jedoch ist dieser sehr gering, sodass er keinen unverhältnismäßigen Einfluss auf etwaige Analyseergebnisse hat. Die T-Werte werden zunächst durch den Wert 100 ersetzt, da so Probleme beim Umrechnen umgangen werden, die durch z.B. NAs entstehen. In keiner der Spalten kommt vorher der Wert 100 vor, sodass nach dem Umrechnen weiterhin klar ist, welche Werte vorher ein T waren. Nach dem Umrechnen haben diese den Wert 2540 und können problemlos mit 0.01 ersetzt werden.

```{r, eval=FALSE, echo=TRUE}

#Datum richtig formatieren
weather$date <- dmy(weather$date)

#Temperatur in Celsius umrechnen
weather <- weather %>%
  mutate(maximum.temperature = round((maximum.temperature-32)*5/9, 2),
         minimum.temperature = round((minimum.temperature-32)*5/9, 2),
         average.temperature = round((average.temperature-32)*5/9, 2))

levels(weather$precipitation)
levels(weather$snow.fall)
levels(weather$snow.depth)

#Niederschlagsmenge formatieren, T-Werte durch 100 ersetzen und zurückformatieren
weather$precipitation <- as.character(weather$precipitation)
weather$precipitation[weather$precipitation == "T"] <- 100
weather$precipitation <- as.double(weather$precipitation)

#Schneefall und -tiefe formatieren, T-Werte durch 100 ersetzten und zurückformatieren
weather$snow.fall <- as.character(weather$snow.fall)
weather$snow.fall[weather$snow.fall == "T"] <- 100
weather$snow.fall <- as.double(weather$snow.fall)

weather$snow.depth <- as.character(weather$snow.depth)
weather$snow.depth[weather$snow.depth == "T"] <- 100
weather$snow.depth <- as.double(weather$snow.depth)

#Regen und Schnee in mm umrechen
weather <- weather %>%
  mutate(precipitation = round(precipitation*25.4, 2),
         snow.fall = round(snow.fall*25.4, 2),
         snow.depth = round(snow.depth*25.4, 2))

#Die ehemaligen T-Werte (jetzt 2540) mit 0.01 ersetzten
max(weather$precipitation)
weather$precipitation[weather$precipitation == 2540] <- 0.01

max(weather$snow.fall)
weather$snow.fall[weather$snow.fall == 2540] <- 0.01

max(weather$snow.depth)
weather$snow.depth[weather$snow.depth == 2540] <- 0.01

# Wetter gespeichert

saveRDS(weather, "Data/weather_daily_2016.rds")

```

```{r}
weather <- readRDS("Data/weather_daily_2016.rds")
```

Die Wetterdaten für das Jahr 2017, werden genauso aufbereitet wie die vorherigen. Die Maßeinheiten Fahrenheit und Inches werden wieder in Celsius und Millimeter umgerechnet. Da die Spalte mit der täglichen Durchschnittstemperatur ausschließlich fehlende Werte enthält, wird sie aus dem Mittelwert der minimal und maximal Temperatur ermittelt. Dies ist ebenfalls das Vorgehen des NCDC. Die Spalten Niederschlagsmenge, Schneefall und Schneetiefe enthalten im Jahr 2017 keine T-Werte wie noch im Jahr 2016 und müssen hier nicht dementsprechend aufbereitet werden.

```{r, eval=FALSE, echo=TRUE}

#Daten laden
weather17 <- read_csv("Data/daily_weather_2017.csv")

#Daten für 2017 auswählen, Spaltennamen dem Vorjahr anpassen und Durchschnittstemperatur berechnen
weather17 <- weather17 %>%
  filter(year(DATE) == 2017) %>%
  select(date = DATE,
         maximum.temperature = TMAX,
         minimum.temperature = TMIN,
         precipitation = PRCP,
         snow.fall = SNOW,
         snow.depth = SNWD) %>%
  mutate(average.temperature = (minimum.temperature+maximum.temperature)/2)

#Fehlende Werte überprüfen
apply(df, 2, function(x) sum(is.na(x)))

#Einheiten umrechnen
weather17 <- weather17 %>%
  mutate(maximum.temperature = round((maximum.temperature-32)*5/9, 2),
         minimum.temperature = round((minimum.temperature-32)*5/9, 2),
         average.temperature = round((average.temperature-32)*5/9, 2),
         precipitation = round(precipitation*25.4, 2),
         snow.fall = round(snow.fall*25.4, 2),
         snow.depth = round(snow.depth*25.4, 2))

#Data Frame wie Vorjahr sortieren
weather17 <- weather17[,c(1:3,7,4:6)]

#Datensatz speichern
saveRDS(weather17, "Data/weather_daily_2017.RDS")
```

```{r}
weather_2017 <- read_rds("Data/weather_daily_2017.RDS")
```


#### Stündliche Wetterdaten

Auch die stündlichen Wetterdaten müssen vor Gebrauch bearbeitet werden. Der Datensatz hat in vielen Spalten sehr viele fehlende Werte. In einem ersten Schritt werden die Spalten ausgesucht, die weiterhin betrachtet werden sollen. Die Spalten Datum, Temperatur in Grad Celsius, Windgeschwindigkeit in Km/h, sowie die binären Spalten Nebel, Regen, Schnee, Hagel, Gewitter und Tornado werden weiterhin genutzt. Die Spalte, die z.B den gefallenen Regen in mm enthält, weißt mehr fehlende Werte auf, als Werte da sind. Da dadurch ein sinnvolles interpolieren der Werte nicht mehr möglich ist, wird diese Variable, sowie einige andere von der weiteren Betrachtung ausgeschlossen. Die Spalten Temperatur und Windgeschwindigkeit weisen einige fehlende Werte auf, die aber aufgrund ihrer geringen Anzahl sinnvoll interpoiert werden können und werden. Hierfür wird die Spline-Interpolation genutzt. Auch in diesem Datensatz wird die Spalte, die Datum und Uhrzeit enthält aufgespalten, sodass Datum und Stunde als getrennte Spalten übrig bleiben. Im letzten Schritt der Aufbereitung werden doppelte Beobachtungen behandelt. Für manche Stunden sind verschiedene Temperaturen und Windgeschwindigkeiten bekannt, die sich jeweils nur marginal unterscheiden. Es wird jeweils der erste gemessene Wert für die weiteren Analysen beibehalten.

```{r, eval= FALSE, echo=TRUE}

#Nur Wetterdaten aus 2016 behalten
hourly_weather <- hourly_weather %>%
  filter(date(pickup_datetime) > "2015-12-31" & date(pickup_datetime) < "2017-01-01")

#Fehlende Werte überprüfen
apply(hourly_weather, 2, function(x) sum(is.na(x)))

#Spalten auswählen
hourly_weather <- hourly_weather %>%
  select(pickup_datetime, tempm, wspdm, fog, rain, snow, hail, thunder, tornado)

#Auf fehlende Werte überprüfen und diese Interpolieren
apply(hourly_weather, 2, function(x) sum(is.na(x)))
hourly_weather[is.na(hourly_weather$tempm),]
hourly_weather$tempm <- na_interpolation(hourly_weather$tempm, option = "spline")
hourly_weather[is.na(hourly_weather$wspdm),]
hourly_weather$wspdm <- na_interpolation(hourly_weather$wspdm, option = "spline")
apply(hourly_weather, 2, function(x) sum(is.na(x)))

#Spalten umbenennen
hourly_weather <- hourly_weather %>%
  rename(datetime = pickup_datetime,
         temp = tempm,
         wspd = wspdm)

#Datum und Stunde trennen
hourly_weather <- hourly_weather %>%
  mutate(date = as_date(datetime),
         hour = hour(datetime))
hourly_weather$datetime <- NULL         

#Reihenfolge der Spalten festlegen und nur einen Messwert je Stunde behalten
hourly_weather <- hourly_weather[,c(9,10,1:8)]
hourly_weather <- hourly_weather %>% distinct(date, hour, .keep_all = T)

#Daten speichern
saveRDS(hourly_weather, "hourly_weather.RDS")

```

Aufgrund der dünnen Datenlage zum stündlichen Wetter im Jahr 2017, werden zwei andere Datensätze für die Jahre 2016 und 2017 genutzt, die das stündliche Wetter enthalten. 

Auch diese müssen zunächst aufbereitet werden. Da sehr viele Variablen vorhanden sind, die zum Teil viele fehlende Werte enthalten werden die stündlichen Daten für die Temperatur an der Messstation in Fahrenheit, die Windgeschwindigkeit in Meilen pro Stunde, sowie die Niederschlagsmenge in inches ausgewählt. Fahrenheit wird in Celsius, Meilen pro Stunde in Kilometer pro Stunde und inches in Millimeter umgerechnet. Kann die Menge des gefallenen Regens nicht gemessen werden, wird dies durch ein T gekennzeichnet. Um mit den Daten problemlos arbeiten zu können, wird das T durch den Wert 0.01 ersetzt. So ist ein numerischer Wert vorhanden, der kennzeichnet, dass Regen gefallen ist. Jedoch ist dieser sehr gering, sodass er keinen unverhältnismäßigen Einfluss auf etwaige Analyseergebnisse hat. Die T-Werte werden zunächst durch den Wert 100 ersetzt, da so Probleme beim Umrechnen umgangen werden, die durch z.B. NAs entstehen. In keiner der Spalten kommt vorher der Wert 100 vor, sodass nach dem Umrechnen weiterhin klar ist, welche Werte vorher ein T waren. Nach dem Umrechnen haben diese den Wert 2540 und können problemlos mit 0.01 ersetzt werden. Sind mehrere Werte für eine Stunde vorhanden, so wird nur der erste Wert weiter berücksichtigt. Verbliebene fehlende Werte werden mit Hilfe der Spline-Interpolation ersetzt. Dies geschieht für die Jahre 2016 und 2017 getrennt. 

```{r, eval=FALSE, echo=TRUE}

#Daten einlesen
df <- read_csv("Data/hourly_weather_new.csv")

#stündliche Daten auswählen
df <- df %>% select(DATE, HourlyAltimeterSetting, HourlyDewPointTemperature, HourlyDryBulbTemperature, 
                    HourlyPrecipitation, HourlyPresentWeatherType, HourlyPressureChange,   HourlyPressureTendency,
                    HourlyRelativeHumidity, HourlySeaLevelPressure, HourlySkyConditions,  HourlyStationPressure,
                    HourlyVisibility, HourlyWetBulbTemperature, HourlyWindDirection, HourlyWindGustSpeed, 
                    HourlyWindSpeed)

#Fehlende Werte überprüfen
apply(df, 2, function(x) sum(is.na(x)))

#Temperatur, Windgeschwindigkeit und Niederschlagsmenge auswählen
df <- df %>% select(DATE, HourlyDryBulbTemperature, HourlyPrecipitation, HourlyWindSpeed)

#Datensatz nach Jahren 2016 und 2017 aufsplitten
df16 <- df %>%
  filter(year(DATE)==2016)
df17 <- df %>%
  filter(year(DATE)==2017)

#Jeweils fehlende Werte überprüfen
apply(df16, 2, function(x) sum(is.na(x)))
apply(df17, 2, function(x) sum(is.na(x)))

#2016
#Datum und Stunde trennen, sowie Uhrzeit als eigene Spalte behalten
df16 <- df16 %>%
  mutate(date = as_date(DATE),
         hour = hour(DATE),
         time = paste(hour(DATE), minute(DATE), second(DATE), sep = ":"))

#Alle Zeilen mit Uhrzeit 23:59:00 entfernen, da letzter Tagesreport mit allen Werten NA
df16 <- df16 %>% 
  filter(time != "23:59:0")
df16$time <- NULL
df16$DATE <- NULL

#Temperatur und Windgeschwindigkeit umrechnen
df16 <- df16 %>%
  mutate(temp = round((HourlyDryBulbTemperature-32)*5/9, 2),
         wdsp = round(HourlyWindSpeed*1.609344, 2))
df16$HourlyDryBulbTemperature <- NULL
df16$HourlyWindSpeed <- NULL

#Niederschlagsmenge umbenennen, Werte mit Buchstaben korrigieren, T durch 1000 ersetzten, in mm umrechnen und die ehemaligen T-Werte in 0.01mm ändern
df16 <- df16 %>%
  rename(precip = HourlyPrecipitation)

df16$precip <- str_replace_all(df16$precip, c("s" = ""))
df16$precip[df16$precip == "T"] <- 1000
df16$precip <- as.double(df16$precip)

df16 <- df16 %>%
  mutate(precip = round(precip*25.4, 2))
max(df16$precip, na.rm = T)
df16$precip[df16$precip == 25400] <- 0.01

#Reihenfolge der Spalten ändern und nur eine Zeile pro Stunde behalten
df16 <- df16[,c(2,3,4,5,1)]
df16 <- df16 %>% 
  distinct(date, hour, .keep_all = T)

#Fehlende Werte interpolieren
df16[,3:5] <- na_interpolation(df16[,3:5], option = "spline")
apply(df16, 2, function(x) sum(is.na(x)))

#2017
#Datum und Stunde trennen, sowie Uhrzeit als eigene Spalte behalten
df17 <- df17 %>%
  mutate(date = as_date(DATE),
         hour = hour(DATE),
         time = paste(hour(DATE), minute(DATE), second(DATE), sep = ":"))

#Alle Zeilen mit Uhrzeit 23:59:00 entfernen, da letzter Tagesreport mit allen Werten NA
df17 <- df17 %>% 
  filter(time != "23:59:0")
df17$time <- NULL
df17$DATE <- NULL

#Temperatur und Windgeschwindigkeit umrechnen
df17 <- df17 %>%
  mutate(temp = round((HourlyDryBulbTemperature-32)*5/9, 2),
         wdsp = round(HourlyWindSpeed*1.609344, 2))
df17$HourlyDryBulbTemperature <- NULL
df17$HourlyWindSpeed <- NULL

#Niederschlagsmenge umbenennen, Werte mit Buchstaben korrigieren, T durch 1000 ersetzten, in mm umrechnen und die ehemaligen T-Werte in 0.01mm ändern
df17 <- df17 %>%
  rename(precip = HourlyPrecipitation)

df17$precip <- str_replace_all(df17$precip, c("s" = ""))
df17$precip[df17$precip == "T"] <- 1000
df17$precip <- as.double(df17$precip)

df17 <- df17 %>%
  mutate(precip = round(precip*25.4, 2))
max(df17$precip, na.rm = T)
df17$precip[df17$precip == 25400] <- 0.01

#Reihenfolge der Spalten ändern und nur eine Zeile pro Stunde behalten
df17 <- df17[,c(2,3,4,5,1)]
df17 <- df17 %>% 
  distinct(date, hour, .keep_all = T)

#Fehlende Werte interpolieren
df17[,3:5] <- na_interpolation(df17[,3:5], option = "spline")
apply(df17, 2, function(x) sum(is.na(x)))

#Daten speichern
saveRDS(df16, "Data/hourly_weather_2016.RDS")
saveRDS(df17, "Data/hourly_weather_2017.RDS")
```

```{r}
hourly_weather_16 <- read_rds("Data/hourly_weather_2016.RDS")
hourly_weather_17 <- read_rds("Data/hourly_weather_2017.RDS")
```

Im weiteren Verlauf dieser Arbeit wird mit den Datensätzen hourly_weather_16 und hourly_weather_2017 gearbeitet.

### Aggregate Data

 Um die erste Forschungsfrage beantworten zu können, müssen die Daten zunächst so aggregiert werden, dass die Anzahl der Fahrten pro Stunde und Station sichtbar werden. Hier werden zwei Datensätze erstellt: Der Erste hält die Anzahl der pro Stunde losgefahrenen Radfahrer je Station fest (hourly_starts). Der Zweite hält die Anzahl der pro Stunde ankommenden Radfahrer je Station fest (hourly_stops). Die Anzahl der Fahrer wird mit der Variable user_count beschrieben. Die Variablen avg_age und avg_tripduration beschreiben jeweils das durchschnittliche Alter in Jahren, sowie die durchschnittliche Fahrtdauer in Sekunden. Die Variable weekend gibt an, ob der jeweilige Tag ein Wochenendtag (Samstag oder Sonntag) ist (1 = ja, 0 = nein). In den Variablen male_user_count, female_user_count und undefined_user_count geben die Anzahl der Fahrer nach Geschlecht gesplittet an. subscriber_count und customer_count geben an, wie viele der Fahrer ein Abo hatten bzw. einen 1-Tages oder 3-Tages-Pass. In den letzten 5 genannten Variablen werden fehlende Werte durch 0 ersetzt, da sich diese durch das Pivotieren der jeweiligen Data Frames ergeben und für die Anzahl der Fahrer stehen.
 
```{r, eval= FALSE, echo=TRUE}

#Datensatz mit getrenntem Datum und Stunde erstellen
bike_q1 <- bike
bike_q1 <- bike_q1 %>%
  mutate(start_date = as_date(starttime),
         start_hour = hour(starttime),
         stop_date = as_date(stoptime),
         stop_hour = hour(stoptime))

#Datensatz für stündliche Starts je Station erstellen und Spalten für die Anzahl der losfahrenden Radfahrer, deren durchschnittliches Alter und die durchschnittliche Fahrtdauer einfügen, sowie Variable für Wochenende
hourly_starts <- bike_q1 %>%
  group_by(start_date, start_hour, start_station_id, start_station_name, start_station_latitude,
           start_station_longitude) %>%
  summarise(user_count = n(),
            avg_age = round(2016 - mean(birth_year)),
            avg_tripduration = mean(tripduration)) %>%
  mutate(weekend = ifelse(wday(start_date) == 6 | wday(start_date) == 7, 1,0))

#Datensatz erstellen der nach Stunde, Station und Geschlecht getrennt Fahrer zählt
gender <- bike_q1 %>%
  group_by(start_date, start_hour, start_station_id, gender) %>%
  summarise(user = n())
gender <- pivot_wider(gender, names_from = gender, values_from = user)

#Beide Datensätze verbinden und redundante Spalten löschen
hourly_starts <- cbind(hourly_starts, gender)
hourly_starts$start_date1 <- NULL
hourly_starts$start_hour1 <- NULL
hourly_starts$start_station_id1 <- NULL

#Datensatz erstellen, der nach Stunde, Station und Nutzertyp getrennt Fahrer zählt
usertype <- bike_q1 %>%
  group_by(start_date, start_hour, start_station_id, usertype) %>%
  summarise(user = n())
usertype <- pivot_wider(usertype, names_from = usertype, values_from = user)

#Beide Datensätze verbinden und redundante Spalten löschen
hourly_starts <- cbind(hourly_starts, usertype)
hourly_starts$start_date1 <- NULL
hourly_starts$start_hour1 <- NULL
hourly_starts$start_station_id1 <- NULL

#Spalten umbenennen
hourly_starts <- hourly_starts %>%
  rename(male_user_count = "1",
         female_user_count = "2",
         undefined_user_count = "0",
         subscriber_count = Subscriber,
         customer_count = Customer)

#Fehlende Werte ersetzen
hourly_starts$male_user_count[is.na(hourly_starts$male_user_count)] <- 0
hourly_starts$female_user_count[is.na(hourly_starts$female_user_count)] <- 0
hourly_starts$undefined_user_count[is.na(hourly_starts$undefined_user_count)] <- 0
hourly_starts$subscriber_count[is.na(hourly_starts$subscriber_count)] <- 0
hourly_starts$customer_count[is.na(hourly_starts$customer_count)] <- 0

#nicht mehr benötigte Datensätze löschen
rm(gender)
rm(usertype)

#Datensatz für stündliche Stops je Station erstellen und Spalten für die Anzahl der losfahrenden Radfahrer, deren durchschnittliches Alter und die durchschnittliche Fahrtdauer einfügen, sowie Variable für Wochenende
hourly_stops <- bike_q1 %>%
  group_by(stop_date, stop_hour, end_station_id, end_station_name, end_station_latitude,
           end_station_longitude) %>%
  summarise(user_count = n(),
            avg_age = round(2016 - mean(birth_year)),
            avg_tripduration = mean(tripduration)) %>%
  mutate(weekend = ifelse(wday(stop_date) == 6 | wday(stop_date) == 7, 1,0))

#Datensatz erstellen der nach Stunde, Station und Geschlecht getrennt Fahrer zählt
gender <- bike_q1 %>%
  group_by(stop_date, stop_hour, end_station_id, end_station_name, end_station_latitude,
           end_station_longitude,gender) %>%
  summarise(user = n())
gender <- pivot_wider(gender, names_from = gender, values_from = user)

#Beide Datensätze verbinden und redundante Spalten löschen
hourly_stops <- cbind(hourly_stops, gender)
hourly_stops$stop_date1 <- NULL
hourly_stops$stop_hour1 <- NULL
hourly_stops$end_station_id1 <- NULL
hourly_stops$end_station_name1 <- NULL 
hourly_stops$end_station_latitude1 <- NULL
hourly_stops$end_station_longitude1 <- NULL

#Datensatz erstellen, der nach Stunde, Station und Nutzertyp getrennt Fahrer zählt
usertype <- bike_q1 %>%
  group_by(stop_date, stop_hour, end_station_id, end_station_name, end_station_latitude,
           end_station_longitude, usertype) %>%
  summarise(user = n())
usertype <- pivot_wider(usertype, names_from = usertype, values_from = user)

#Beide Datensätze verbinden und redundante Spalten löschen
hourly_stops <- cbind(hourly_stops, usertype)
hourly_stops$stop_date1 <- NULL
hourly_stops$stop_hour1 <- NULL
hourly_stops$end_station_id1 <- NULL
hourly_stops$end_station_name1 <- NULL 
hourly_stops$end_station_latitude1 <- NULL
hourly_stops$end_station_longitude1 <- NULL

#Spalten umbenennen
hourly_stops <- hourly_stops %>%
  rename(male_user_count = "1",
         female_user_count = "2",
         undefined_user_count = "0",
         subscriber_count = Subscriber,
         customer_count = Customer)

#Fehlende Werte ersetzen
hourly_stops$male_user_count[is.na(hourly_stops$male_user_count)] <- 0
hourly_stops$female_user_count[is.na(hourly_stops$female_user_count)] <- 0
hourly_stops$undefined_user_count[is.na(hourly_stops$undefined_user_count)] <- 0
hourly_stops$subscriber_count[is.na(hourly_stops$subscriber_count)] <- 0
hourly_stops$customer_count[is.na(hourly_stops$customer_count)] <- 0

#Nicht mehr benötigte Datensätze löschen
rm(gender)
rm(usertype)

#Aggregierte Datensätze speichern
saveRDS(hourly_starts, "Data/bike_q1_starts.rds")
saveRDS(hourly_stops, "Data/bike_q1_stops.rds")

```

```{r}
hourly_starts <- readRDS("Data/bike_q1_starts.rds")
hourly_stops <- readRDS("Data/bike_q1_stops.rds")

```

 Um die Daten für die zweite Frage zu aggregieren, müssen die Abfahrten und Ankünfte pro Station pro Tag zusammengefasst werden. Deswegen werde ich als erstes das Datum extrahieren damit ich damit nach dem Tag aggregieren kann.
 
```{r, eval= FALSE, echo=TRUE}

# Kopiere Datensatz in Question2 Datensatz und wandel Date/time Spalte in Date spalte um und füge Sie hinzu
bike_q2 <- bike
bike_q2 <- bike_q2 %>%
  mutate(date_start=date(bike_q2$starttime),
         date_stop=date(bike_q2$stoptime))


# Extrahiere das Gender aus dem Datensatz für die Starts
bike_q2_starts_gender <- bike_q2 %>%
  group_by(date_start, start_station_name, start_station_latitude, start_station_longitude, gender) %>%
  summarise( anzahl = n() )

# Extrahiere das Gender aus dem Datensatz für die Stops
bike_q2_stops_gender <- bike_q2 %>%
  group_by(date_stop, end_station_name, end_station_latitude, end_station_longitude, gender) %>%
  summarise( anzahl = n() )

# Tidy Gender Tabelle für starts und stops
gender_q2_starts <- pivot_wider(bike_q2_starts_gender, names_from = gender, values_from = anzahl)

gender_q2_stops <- pivot_wider(bike_q2_stops_gender, names_from = gender, values_from = anzahl)


# Aggregiere Starts und Stops nach Datum und Station

bike_q2_starts <- bike_q2 %>%
  group_by(date_start, start_station_name, start_station_latitude, start_station_longitude) %>%
  summarise(avr_age = round(2016-mean(birth_year)), avg_tripduration = mean(tripduration), anzahl = n() )
bike_q2_stops <- bike_q2 %>%
  group_by(date_stop, end_station_name, end_station_latitude, end_station_longitude) %>%
  summarise(avr_age = round(2016-mean(birth_year)), avg_tripduration = mean(tripduration), anzahl = n() )


# Füge den agregierten Tabellen das Gender hinzu

bike_q2_starts_rdy <- left_join(bike_q2_starts, gender_q2_starts, by = c("date_start", "start_station_name" ))
bike_q2_stops_rdy <- left_join(bike_q2_stops, gender_q2_stops, by = c("date_stop", "end_station_name" ))

# Nicht benötigte Spalten gelöscht und NAs in 0 umgewandelt weil wir wissen das diese 0 sind

bike_q2_starts_rdy$start_station_latitude.y <- NULL
bike_q2_starts_rdy$start_station_longitude.y <- NULL

bike_q2_starts_rdy[is.na(bike_q2_starts_rdy$`1`),"1"] <- 0
bike_q2_starts_rdy[is.na(bike_q2_starts_rdy$`2`),"2"] <- 0
bike_q2_starts_rdy[is.na(bike_q2_starts_rdy$`0`),"0"] <- 0

bike_q2_starts_rdy <- rename(bike_q2_starts_rdy, male = "1")
bike_q2_starts_rdy <- rename(bike_q2_starts_rdy, woman = "2")
bike_q2_starts_rdy <- rename(bike_q2_starts_rdy, undefined = "0")


bike_q2_stops_rdy$end_station_latitude.y <- NULL
bike_q2_stops_rdy$end_station_longitude.y <- NULL

bike_q2_stops_rdy[is.na(bike_q2_stops_rdy$`1`),"1"] <- 0
bike_q2_stops_rdy[is.na(bike_q2_stops_rdy$`2`),"2"] <- 0
bike_q2_stops_rdy[is.na(bike_q2_stops_rdy$`0`),"0"] <- 0

bike_q2_stops_rdy <- rename(bike_q2_stops_rdy, male = "1")
bike_q2_stops_rdy <- rename(bike_q2_stops_rdy, woman = "2")
bike_q2_stops_rdy <- rename(bike_q2_stops_rdy, undefined = "0")

rm(bike_q2, bike_q2_starts, bike_q2_starts_gender, bike_q2_stops, bike_q2_stops_gender, gender_q2_starts, gender_q2_stops)


saveRDS(bike_q2_starts_rdy, "Data/bike_q2_starts.rds")
saveRDS(bike_q2_stops_rdy, "Data/bike_q2_stops.rds")

```

```{r}
q2_starts <- read_rds("Data/bike_q2_starts.rds")
q2_stops <- read_rds("Data/bike_q2_stops.rds")
```


## Visualization

### Basic Visualizations

Um ein Gefühl für die Daten zu bekommen und erste Einblicke in deren Beschaffenheit zu erhalten, werden zunächst einfache Visualisierungen genutzt.

```{r include=FALSE}
library(ggplot2)
library(gridExtra)
theme_set(theme_minimal())
```

Zunächst werden die monatlichen Nutzerzahlen betrachtet.

```{r}
q2_starts %>%
  group_by(month = month(date_start, label = T)) %>%
  summarise(user_sum = sum(anzahl)) %>%
  ggplot(aes(month, user_sum)) +
  geom_line(colour = "cadetblue", size = 0.75, group = 1) +
  geom_point(colour = "cadetblue", size = 1.5) +
  labs(title = "Nutzeranzahl Pro Monat", y = "Nutzeranzahl",
       x = "Monat")
```

Diese entsprechen weitestgehend den Erwartungen. In den Wintermonaten sind die Nutzerzahlen wesentlich geringer als in den Sommermonaten. Ab Februar steigen die Nutzerzahlen bis September stetig an, mit Ausnahme eines Einbruchs im Juli. Dieser kann verschiedene Gründe haben. Zum Einen könnten viele Menschen ihren Sommerurlaub nehmen und verreisen oder die Temperaturen könnten so hoch sein, dass Radfahren als unangenehm empfunden wird.

Als nächstes sollen die monatlichen Nutzerzahlen von Männern und Frauen verglichen werden, um zu sehen, ob Unterschiede bestehen.

```{r}
q2_starts %>%
  group_by(month = month(date_start, label = T)) %>%
  summarise(male_sum = sum(male),
            female_sum = sum(woman)) %>%
  ggplot() +
  geom_line(aes(month, male_sum, colour = "cadetblue"), size = 0.75, group = 1) +
  geom_line(aes(month, female_sum, colour = "cadetblue4"), size = 0.75, group = 1) +
  scale_colour_manual(name = "Gender", values = c("cadetblue" = "cadetblue", "cadetblue4" = "cadetblue4"),
                      labels = c("Male", "Female")) +
  labs(title = "Nutzeranzahl Pro Monat Nach Geschlecht",
       y = "Nutzeranzahl", x = "Monat")
```

Auch nach der Trennung der Nutzerzahlen nach Geschlecht ist das vorherige Muster noch sichtbar. Jedoch ist klar erkennbar, dass deutlich mehr Männer das Fahrradangebot nutzen als Frauen. Hier kann weitere Forschung seitens des Unternehmens oder der Stadt New York betrieben werden, um herauszufinden, warum im Verhältnis zu den Männern so wenige Frauen das Angebot nutzen. Hieraus können dann Strategien abgeleitet werden, um mehr Frauen anzusprechen und sie als Kunden zu gewinnen.

In einem nächsten Schritt werden die durchschnittlichen Nutzerzahlen, sowie die durchschnittliche Fahrtdauer pro Wochentag betrachtet.

```{r}
p <- q2_starts %>%
  group_by(wday = wday(date_start, label = T, abbr = F, week_start = 1)) %>%
  summarise(user_mean = round(sum(anzahl)/365)) %>%
  ggplot(aes(wday, user_mean)) +
  geom_line(colour = "cadetblue", size = 0.75, group = 1) +
  geom_point(colour = "cadetblue", size = 1.5) +
  labs(title = "Durschnittliche Nutzeranzahl Pro Wochentag", y = "Durchschnittliche Nutzeranzahl",
       x = "Wochentag")

p1 <- q2_starts %>%
  group_by(wday = wday(date_start, label = T, abbr = F, week_start = 1)) %>%
  summarise(mean_tripduration = mean(avg_tripduration)) %>%
  ggplot(aes(wday, mean_tripduration)) +
  geom_line(colour = "cadetblue", size = 0.75, group = 1) +
  geom_point(colour = "cadetblue", size = 1.5) +
  labs(title = "Durschnittliche Fahrtdauer Pro Wochentag", y = "Durchschnittliche Fahrtdauer",
       x = "Wochentag")

grid.arrange(p, p1, nrow = 2)
```

Betrachtet man zunächst die Nutzerzahlen pro Wochentag, so fällt auf, dass diese unter der Woche höher sind als am Wochenende. Dies lässt darauf schließen, dass viele Kuden das Rad nutzen, um zur Arbeit, zur Schule oder zu Universität zu fahren.
Im Gegensatz hierzu steht die durchschnittliche Dauer einer Fahrt. Die Fahrtdauer ist mit 15 Minuten am Samstag am höchsten. Dies könnte darauf schließen lassen, dass die Fahrtwege, die zur Arbeit zurückgelegt werden etwas kürzer sind, als diejenigen, die zu Freizeitzwecken zurückgelegt werden. Es kann jedoch nicht abschließend geklärt werden, ob dies tatsächlich der Fall ist, da über die Kunden ausßer ihrem Alter, Geschlecht und Abo-Status nichts bekannt ist. Des Weiteren sind die Unterschiede in der Fahrtdauer mit einigen Minuten nicht sehr groß.

Um weitere Anhaltspunkte für die Nutzung zu erhalten, werden die Nutzerzahlen eines Tages in Stundenintervallen betrachtet.

```{r}
hourly_starts %>%
  group_by(start_hour) %>%
  summarise(user_mean = round(sum(user_count)/365)) %>%
  ggplot(aes(start_hour, user_mean)) +
  geom_line(colour = "cadetblue4", size = 0.75) +
  geom_point(colour = "cadetblue4", size = 1.5) +
  labs(title = "Durchschnittliche Nutzeranzahl Pro Stunde", x = "Stunde", 
       y = "Durchschnittliche Nutzeranzahl")
```

Auch hier kann ein Muster erkannt werden. Von 5 Uhr bis 9 Uhr morgens steigen die Nutzerzahlen und fallen danach ab, um von 15 bis 17 Uhr wieder anzusteigen. Dies spricht für die zuvor geäußerte Vermutung, dass viele Kunden die Fahrräder für den Arbeitsweg nutzen. Am Vormittag und Nachmittag sind die Nutzerzahlen am höchsten. Dies kann den Arbeitsbeginn und das Arbeitsende markieren.

Als nächstes werden nun die Nutzerzahlen getrennt nach Abonnement (usertype) betrachtet, um festzustellen, ob Unterschiede in der täglichen Nutzung bestehen.

```{r}
hourly_starts %>%
  group_by(start_hour) %>%
  summarise(subscriber_mean = round(sum(subscriber_count)/365),
            customer_mean = round(sum(customer_count)/365)) %>%
  ggplot() +
  geom_line(aes(start_hour, subscriber_mean, colour = "cadetblue"), size = 0.75) +
  geom_line(aes(start_hour, customer_mean, colour = "cadetblue4"), size = 0.75) +
  scale_colour_manual(name = "User Type", values = c("cadetblue" = "cadetblue", "cadetblue4" = "cadetblue4"),
                      labels = c("Subscriber", "Customer")) +
  labs(title = "Durchschnittliche Nutzeranzahl Pro Stunde Nach Abonnement",
       y = "Durchschnittliche Nutzeranzahl", x = "Stunde")
```

Es ist zu sehen, dass für die Nutzergruppe, die ein jährliches Abo hat (Subscriber), das stündliche Muster erhalten bleibt. Jeweils morgens und nachmittags zu Arbeitsbeginn und -ende nutzen die meisten Kunden das Angebot. Die Nutzergruppe Customer, die nur einen ein- oder drei-Tages-Pass nutzen ist stark unterrepräsentiert. Dies kann darauf schließen lassen, dass das Angebot eher von Personen genutzt wird, die dies regelmäßig nutzen wollen, wie z.B. für den Arbeitsweg, und eher nicht für z.B. Tagesausflüge. Ähnlich wie bei den starken Unterschieden in der Nutzung bei den Geschlechtern, kann es hier lohnenswert sein die Hintergründe zu betrachten und weiter zu erforschen, um neue Nutzergruppen zu aquirieren.

Um mehr über die Nutzer zu erfahren, wird nun das durchschnittliche Alter der Nutzer pro Stunde betrachtet.

```{r}
hourly_starts %>%
  group_by(start_hour) %>%
  summarise(mean_age = mean(avg_age)) %>%
  ggplot(aes(start_hour, mean_age)) +
  geom_line(colour = "cadetblue", size = 0.75) +
  geom_point(colour = "cadetblue", size = 1.5) +
  labs(title = "Durschnittliches Alter Der Nutzer Pro Stunde", y = "Durchschnittliches Alter",
       x = "Stunde")
```

Es ist zu erkennen, dass die Nutzer am Abend und in der Nacht jünger sind, als die Nutzer am Tag. Hierfür kann es verschiedene Gründe geben. Zum Beispiel kann es sein, dass jüngere Kunden eher nachts ausgehen und für den Heimweg das Rad nehmen. Dies kann allerdings nicht abschließend bestätig werden, da zu wenig über die Nutzer bekannt ist. Es kann sich hier ebenfalls lohnen, die Altersstruktur der Nutzer genauer zu betrachten, um neue Nutzergruppen zu identifizieren. Es könnte beispielsweise sein, dass sich die Nutzer, die mit dem Rad zu Arbeit fahren, in mehr als der Nutzungsart von denjenigen unterscheiden, die nachts mit dem Rad von einer Feier nach Hause fahren. In der Konsequenz können die unterschiedlichen Nutzergruppen besser abgestimmt auf ihr Alter und den Nutzungszweck angesprochen werden.

In einer weiteren Grafik wird die durchschnittliche Fahrtdauer pro Stunde betrachtet.

```{r}
hourly_starts %>%
  group_by(start_hour) %>%
  summarise(mean_tripduration = mean(avg_tripduration)) %>%
  ggplot(aes(start_hour, mean_tripduration)) +
  geom_line(colour = "cadetblue", size = 0.75) +
  geom_point(colour = "cadetblue", size = 1.5) +
  labs(title = "Durschnittliche Fahrtdauer Pro Stunde", y = "Durchschnittliche Fahrtdauer",
       x = "Stunde")
```

Die durchschnittliche Fahrtzeit pro Stunde unterscheidet sich nur leicht. Sie bewegt sich in einem Rahmen von 700 bis 900 Sekunden bzw. 11.5 und 15 Minuten. Am längsten ist die Fahrtzeit in der Zeit zwischen 2 und 3 Uhr morgens. Dies könnte darauf hindeuten, dass die Kunden nachts eher weiter entfernt von ihrem Wohnort aufhalten.

In der letzten Grafik werden die Nutzerzahlen je Durchschnittstemperatur betrachtet.

```{r message=FALSE, warning=FALSE}
q2_starts %>%
  full_join(weather, by = c("date_start" = "date")) %>%
  group_by(average.temperature) %>%
  summarise(user_sum = sum(anzahl)) %>%
  ggplot(aes(x = average.temperature, y = user_sum)) +
  geom_point(colour = "cadetblue3", size = 1.5) +
  geom_smooth(colour = "cadetblue4", se = F) +
  labs(title = "Nutzeranzahl je Durchschnittstemperatur", x = "Durchschnittstemperatur in Grad Celsius",
       y = "Nutzeranzahl") +
  ylim(0,400000)
```

Es ist eine klare Tendenz erkennbar. Mit steigender Temperatur steigt die Anzahl der Nutzer. Ab einer gewissen Temperatur sinken die Nutzerzahlen wieder. Hier lässt sich vermuten, dass es den Nutzern zu warm ist, um Fahrrad zu fahren.

### Ein Tag in New York

Um die Nutzerströme zwischen den Stationen zu den verschiedenen Tageszeiten sichtbar zu machen, folgt eine animierte Grafik, die jeden Nutzer zeigt, der am 01.06.2016 ein Rad entliehen und wieder abgegeben hat.

```{r, eval=FALSE, echo=TRUE}
library(geojsonio)
library(move)
library(moveVis)

#load data for june 2016
df6 <- read_rds("Data/Bike_Data_Jun.RDS")
df6$starttime <- mdy_hms(df6$starttime)
df6$stoptime <- mdy_hms(df6$stoptime)

#add the column track_id and tidy the data so that each start and each stop gets its own row
df6 <- rowid_to_column(df6, "track_id")
geo <- df6 %>%
  pivot_longer(cols = c(starttime, stoptime), names_to = "datetime", values_to = "timestamp")
geo1 <- df6 %>%
  pivot_longer(cols = c(start_station_id, end_station_id), names_to = "station", values_to = "station_id")
geo2 <- df6 %>%
  pivot_longer(cols = c(start_station_latitude, end_station_latitude), names_to = "station", values_to = "station_lat")
geo3 <- df6 %>%
  pivot_longer(cols = c(start_station_longitude, end_station_longitude), names_to = "station", values_to = "station_long")
geo_rdy <- cbind(geo, geo1[,c("station_id")], geo2[,c("station_lat")], geo3[,c("station_long")])
geo_rdy <- as_tibble(geo_rdy)
geo_final <- geo_rdy %>% dplyr::select(track_id, timestamp, station_id, station_lat, station_long)

#remove unnecessary data frames
rm(geo)
rm(geo1)
rm(geo2)
rm(geo3)
rm(geo_rdy)

#process the data, so that only complete cases from 01.06.2016 with a distinct start- and end-station are included
geo_final <- geo_final %>%
  filter(date(timestamp) == "2016-06-01")
geo_unique <- unique(c(geo_final[,1], geo_final[,3], geo_final[,4], geo_final[,5]))
geo_unique <- as.data.frame(geo_unique)
colnames(geo_unique) <- c("track_id", "station_id", "station_lat", "station_long")
geo_unique <- geo_unique %>% distinct()
geo_unique <- geo_unique[geo_unique$track_id %in% names(which(table(geo_unique$track_id) == 2)), ]
geo_final <- inner_join(geo_unique, geo_final, by = c("track_id", "station_id", "station_lat", "station_long"))

#import the map, onto which will be projected
map <- geojson_read("Data/new-york-city-boroughs.geojson", what = "sp")

#create an object of class move
geo_move <- df2move(geo_final, proj = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0", 
        track_id = "track_id", x = "station_long", y = "station_lat", time = "timestamp")

#align the movement on one scale 
geo_aligned <- align_move(geo_move, unit = "hours")

#create frames for movement
geo_frames <- frames_spatial(geo_aligned, map_service = "osm", map_type = "watercolor", alpha = 0.5) %>%
  add_labels(x = "Longitude", y = "Latitude", title = "Movement Of Cyclists", 
             subtitle = "First Week Of June 2016") %>% 
  add_northarrow() %>%
  add_scalebar() %>%
  add_timestamps(geo_aligned, type = "label") %>%
  add_progress()

#animate frames and save as .gif
animate_frames(geo_frames, out_file = "movement_of_cyclists_6_2016.gif")
```

Die hier erstellte Grafik wird nicht weiter genutzt und betrachtet. Dies hat verschiedene Gründe. Zum Einen ist die Visualisierung aufgrund der Menge an Datenpunkten, die sich auf einem relativ kleinen Ausschnitt der Karte befinden zu unübersichtlich. Zum Anderen ist auch kein klares Fahrtenmuster erkennbar. Dies zum Teil aufgrund der Unübersichtlichkeit, aber auch, da fast alle Stationen im Stadtteil Manhattan liegen und die Fahrten recht gleichmäßig auf alle Stationen verteilt sind.

### heat map / choropleth map / interactive map / animierte map

Als erstes laden wir einige Plugins zum visualisieren von Maps und Geodaten

```{r}
# load packeges
library(tigris)
library(leaflet)
library(sp)
library(maptools)
library(broom)
library(httr)
library(rgdal)
```


Nun kreiere ich einen Stations Dataframe mit den Gesamtnutzerzahlen für das gesamte Jahr.

```{r}
# create stations data frame

 stations <- q2_starts %>%
  group_by(start_station_name, start_station_latitude.x, start_station_longitude.x) %>%
  summarise( count = n(), totalusers = sum(anzahl))

stations$count <- NULL
```



Mit der GET funktion hole ich mir die verschiedenen Neighboorhoods als GeoJSON von einem Link und speicherere diese als GEOJSON.

```{r, eval=FALSE, echo=TRUE}
# get the neighborhoods


r <- GET('http://data.beta.nyc//dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/35dd04fb-81b3-479b-a074-a27a37888ce7/download/d085e2f8d0b54d4590b1e7d1f35594c1pediacitiesnycneighborhoods.geojson')

nyc_neighborhoods <- readOGR(content(r,'text'), 'OGRGeoJSON', verbose = F)


saveRDS(nyc_neighborhoods, "Data/GEOJSON.rds")
```


GeoJSON laden, umformen für GGPLOT und einmal ausgeben

```{r}
nyc_neighborhoods <- readRDS("Data/GEOJSON.rds")

# tidy neighbourhoods from spatial polygon data frame into plain data frame to use it with ggplot

nyc_neighborhoods_df <- tidy(nyc_neighborhoods)


ggplot() + 
  geom_polygon(data=nyc_neighborhoods_df, aes(x=long, y=lat, group=group), color="blue", fill=NA)

```


Ausgabe der Spartial Data mit LEaflet

```{r}
# use leaflet with neighbourhoods

leaflet(nyc_neighborhoods) %>%
  addTiles() %>% 
  addPolygons(popup = ~neighborhood) %>%
  addProviderTiles("CartoDB.Positron")

```


Einmal Stations als points definieren und in einen spatial polygon dataframe umformen

```{r}
# load stations as points

points <- stations %>%
  select(start_station_longitude.x, start_station_latitude.x, totalusers)

points$start_station_name <- as.factor(points$start_station_name)

points <- as.data.frame(points)

# format points into a spatial polygon data frame

points_spdf <- points
coordinates(points_spdf) <- ~start_station_longitude.x + start_station_latitude.x
proj4string(points_spdf) <- proj4string(nyc_neighborhoods)
matches <- over(points_spdf, nyc_neighborhoods)
points <- cbind(points, matches)
```

Einmal die Karte angucken mit leaflet und eingetrgenene Stationen und Neighborhoods mit den Nutzerzahlen


```{r}
# group by neighborhood

points_by_neighborhood <- points %>%
  group_by(neighborhood) %>%
  summarize(userperneighborhood=sum(totalusers))

map_data <- geo_join(nyc_neighborhoods, points_by_neighborhood, "neighborhood", "neighborhood")

pal <- colorNumeric(palette = "RdBu",
                    domain = range(map_data@data$userperneighborhood, na.rm=T))

# look on the map with the stations and neighborhoods

leaflet(map_data) %>%
  addTiles() %>% 
  addPolygons(fillColor = ~pal(userperneighborhood), popup = ~neighborhood) %>% 
  addMarkers(~start_station_longitude.x, ~start_station_latitude.x, popup = ~start_station_name, data = points) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setView(-73.98, 40.75, zoom = 13) %>%

addLegend( pal=pal, values=~userperneighborhood, opacity=0.9, title = "Totalusers", position = "bottomright" )
```

Die Karte ohne die Stationen mit nur den Neighborhoods und der einfärbung der Neighborhoods nach Gesamtnutzerzahlen

```{r}

# map ohne marker

leaflet(map_data) %>%
  addTiles() %>% 
  addPolygons(fillColor = ~pal(userperneighborhood), popup = ~neighborhood) %>% 
  addProviderTiles("CartoDB.Positron") %>%
  setView(-73.98, 40.75, zoom = 13) %>%

addLegend( pal=pal, values=~userperneighborhood, opacity=0.9, title = "Totalusers", position = "bottomright" )

```


Eine Karte mit Stationen ohne NEighborhoods aber mit einfärbung der Stationen nach Nutzerzahlen


```{r}


# Create a color palette with handmade bins.
mybins <- seq(1000, 50000, by=5000)
mypalette <- colorBin( palette="YlOrBr", domain=stations$totalusers, na.color="transparent", bins=mybins)

# Prepare the text for the tooltip:
mytext <- paste(
   "Name: ", stations$start_station_name, "<br/>", 
   "Totalusers: ", stations$totalusers, sep="") %>%
  lapply(htmltools::HTML)

 

# create Map
m <- leaflet(stations) %>% 
  addTiles()  %>% 
  setView( lat=40, lng=-74 , zoom=4) %>%
  addProviderTiles("Esri.WorldImagery") %>%
  addCircleMarkers(~start_station_longitude.x, ~start_station_latitude.x, fillColor = ~mypalette(totalusers), fillOpacity = 0.7, color="white", radius=8, stroke=FALSE,
                   label = mytext,
                   labelOptions = labelOptions( style = list("font-weight" = "normal", padding = "3px 8px"), textsize = "13px",
                                                direction = "auto")) %>%
  addLegend( pal=mypalette, values=~totalusers, opacity=0.9, title = "Totalusers", position = "bottomright" )

# load map

m 
```


Ich versuche nun die Nutzerzahlen übers Jahr in den verschiedenen Borrows zu animieren.

```{r}
library(leaflet.minicharts)

basemap <- leaflet(map_data) %>%
  addTiles() %>% 
  addPolygons(popup = ~neighborhood) %>%
  addProviderTiles("CartoDB.Positron")



basemap %>%
  addMinicharts(
    q2_starts$start_station_longitude.x, q2_starts$start_station_latitude.x,
    type = "auto",
    chartdata = q2_starts$anzahl,
    time = q2_starts$date_start,
    colorPalette = colors,
    transitionTime = 0.1,
  ) %>%
  setView(-73.98, 40.75, zoom = 13)


?addMinicharts
```


Neuer versuch mit Daten die nach den Neighborhoods aggregiert wurden. Dazu aggregiere ich erstmal einnen anderen Datensatz.

```{r}

newTest <- bike
newTest <- newTest %>%
  mutate(start_date = as_date(starttime))

newTest <- newTest %>%
  left_join(points, by = c("start_station_name"="start_station_name",
                           "start_station_latitude"="start_station_latitude.x",
                           "start_station_longitude"="start_station_longitude.x"))

newTest <- newTest %>%
  mutate(stop_date=date(newTest$stoptime))

newTest$totalusers <- NULL
newTest$X.id <- NULL
newTest$usertype <- NULL
newTest$start_hour <- NULL
newTest$starttime <- NULL
newTest$stoptime <- NULL
newTest$boroughCode <- NULL
newTest$borough <- NULL


newTest$neighborhood[newTest$start_station_id==160 & is.na(newTest$neighborhood)] <- "Murray Hill"


newTest$neighborhood <- fct_explicit_na(newTest$neighborhood)


newTest_agg1 <- newTest %>%
  group_by(neighborhood) %>%
  summarise(long = median(start_station_longitude), lat = median(start_station_latitude))

newTest <- left_join(newTest, newTest_agg1, by = "neighborhood")

newTest_agg <- newTest %>%
  group_by(neighborhood, start_date, long, lat) %>%
  summarise(users = n())


```

Nun versuche ich die Animation mit dem neuen Datensatz.

```{r}
basemap %>%
  addMinicharts(
    newTest_agg$long, newTest_agg$lat,
    type = "auto",
    chartdata = newTest_agg$users,
    showLabels = TRUE,
    fillColor = pal,
    width = 45,
    time = newTest_agg$start_date,
    colorPalette = colors,
    transitionTime = 0.1,
  ) %>%
  setView(-73.98, 40.75, zoom = 13)
```

```{r}
rm(points_spdf, points_by_neighborhood, stations, newTest_agg1, matches, nyc_neighborhoods, nyc_neighborhoods_df)
```



## Analytics

### Hourly Data

Um die erste Forschungsfrage beantworten zu können, wird der Datensatz hourly_starts noch weiter angepasst, sowie ein weiterer Datensatz erstellt, der die stündlichen Starts nach den Bezirken (neighborhoods) gruppiert und mehrere Stationen zusammenfasst.

Zunächst wird ein Datensatz erstellt, der nach Bezirken gruppiert ist. Hierfür wird der bike Datensatz mit dem Datensatz gejoined, der die Stationen und Bezirke enthält. Aufgrund von Überschneidungen in den Längen- und Breitengraden ist für die Station Nr. 160 in einigen Stunden kein Bezirk vorhanden. Dieser ist aber bekannt und wird eingesetzt. Hiernach gibt es noch 3 fehlende Bezirkswerte. Diese gehören zu einer Station ohne Koordinaten und werden mit dem Bezirk "missing" ersetzt um explizite fehlende Werte zu erhalten. Der vorhandene Datensatz wird nach Bezirken gruppiert und so aufbereitet wie im Abschnitt *Preprocessing*. Hiernach wird die Variable weekday für den Wochentag erstellt, sowie die Variable Holiday, die angibt, ob es sich um einen Feiertag handelt. Es folgt ein Join mit dem Datensatz hourly_weather, der stündliche Wetterdaten enthält. Da zwischen dem 23.01. und 26.01.2016 keine Fahrten stattfinden, werden die entsprechenden Zeilen mit den Wetterdaten gelöscht. Es fehlen auch Wetterdaten für einige Stunden, die mit Hilfe der Spline-Interpolation sinnvoll geschätzt und ersetzt werden können. Die zuvor eingefügte Variable Wochenende wird wieder aus dem Datensatz entfernt, da das Muster, dass am Wochenende wesentlich weniger Nutzer die Räder fahren bereits über die Variable Weekday abgedeckt wird.

```{r, eval=FALSE, echo=TRUE}

#Bike Daten in einen extra Data Frame laden und Datum und Stunde trennen
df <- bike
df <- df %>%
  mutate(start_date = as_date(starttime),
         start_hour = hour(starttime))

#Bezirke für Stationen hinzufügen
points1 <- points %>% 
  select(start_station_name, start_station_latitude.x, start_station_longitude.x, neighborhood)
df <- df %>%
  left_join(points1, by = c("start_station_name"="start_station_name",
                           "start_station_latitude"="start_station_latitude.x",
                           "start_station_longitude"="start_station_longitude.x"))

#Fehlende Werte überprüfen
apply(df, 2, function(x) sum(is.na(x)))

#Überprüfen wo der Bezirk fehlt
df[is.na(df$neighborhood),]

#Teilweise fehlt der Bezirk für Station 160, manuell ersetzen, da bekannt
df$neighborhood[df$start_station_id==160 & is.na(df$neighborhood)] <- "Murray Hill"

#Für Station 3240 kein Bezirk bekannt, fehlende Werte explizit machen
df$neighborhood <- fct_explicit_na(df$neighborhood)

#Nach Bezirk gruppieren und Nutzerzahlen aufsummieren
hourly_starts_nh <- df %>%
  group_by(start_date, start_hour, neighborhood) %>%
  summarise(user_count = n()) %>%
  mutate(weekend = ifelse(wday(start_date) == 6 | wday(start_date) == 7, 1,0))

#Variable für Wochentag einfügen
hourly_starts_nh <- hourly_starts_nh %>%
  mutate(weekday = wday(start_date, abbr = F, label = T, week_start = 1))
hourly_starts_nh$weekday <- factor(hourly_starts_nh$weekday, ordered = FALSE)

#Variable für Feiertag einfügen
hourly_starts_nh$holiday <- ifelse(isBizday(as.timeDate(hourly_starts_nh$start_date), holidayNYSE(2016))==FALSE,1,0)

#Wetterdaten hinzufügen
hourly_starts_nh <- hourly_starts_nh %>%
  full_join(hourly_weather_16, by = c("start_date" = "date", "start_hour" = "hour"))

#Fehlende Werte überprüfen
apply(hourly_starts_nh, 2, function(x) sum(is.na(x)))
hourly_starts_nh[is.na(hourly_starts_nh$neighborhood),] #vom 23.01.-26.01. keine fahrten

#Nicht genutzte Wetterdaten entfernen
hourly_starts_nh <- hourly_starts_nh[complete.cases(hourly_starts_nh[,3:7]),]

#Fehlende Werte überprüfen und interpolieren
apply(hourly_starts_nh, 2, function(x) sum(is.na(x)))
hourly_starts_nh[,8:10] <- na_interpolation(hourly_starts_nh[,8:10], option = "spline")

#Variable Weekend entfernen, da Wochentage für Betrachtung ausreichen
hourly_starts_nh$weekend <- NULL

#Daten speichern
saveRDS(hourly_starts_nh, "Data/q1_starts_nh.RDS")

```

```{r}
hourly_starts_nh <- readRDS("Data/q1_starts_nh.RDS")
```

Um stationsgenaue Vorhersagen zu treffen, wird der Datenssatz hourly_starts ebenfalls weiter aufbereitet. Wie zuvor werden die Variablen Wochentag und Feiertag eingefügt, sowie die Wetterdaten hinzugefügt.

```{r}
#Variable für Wochentag einfügen
hourly_starts <- hourly_starts %>%
  mutate(weekday = wday(start_date, abbr = F, label = T, week_start = 1))
hourly_starts$weekday <- factor(hourly_starts$weekday, ordered = FALSE)

#Variable für Feiertag einfügen
hourly_starts$holiday <- ifelse(isBizday(as.timeDate(hourly_starts$start_date), holidayNYSE(2016))==FALSE,1,0)

#Wetterdaten hinzufügen
hourly_starts <- hourly_starts %>%
  full_join(hourly_weather_16, by = c("start_date" = "date", "start_hour" = "hour"))

#Fehlende Werte überprüfen
apply(hourly_starts_nh, 2, function(x) sum(is.na(x)))
hourly_starts[is.na(hourly_starts$start_station_name),] #vom 23.01.-26.01. keine fahrten

#Nicht mehr benötigte Variablen entfernen
hourly_starts$start_station_id <- NULL
hourly_starts$start_station_latitude <- NULL
hourly_starts$start_station_longitude <- NULL
hourly_starts$avg_age <- NULL
hourly_starts$avg_tripduration <- NULL
hourly_starts$male_user_count <- NULL
hourly_starts$female_user_count <- NULL
hourly_starts$undefined_user_count <- NULL
hourly_starts$subscriber_count <- NULL
hourly_starts$customer_count <- NULL
hourly_starts$weekend <- NULL

#Nicht genutzte Wetterdaten entfernen
hourly_starts <- hourly_starts[complete.cases(hourly_starts[,3:6]),]

#Fehlende Werte überprüfen und interpolieren
apply(hourly_starts, 2, function(x) sum(is.na(x)))
hourly_starts[,7:9] <- na_interpolation(hourly_starts[,7:9], option = "spline")
```

### Analytics2

Als erstes den Datensatz mit dem Wetter joinen. Dazu bennen wir die Datumsspalte gleich in beiden Datenframes.

```{r}
q2_starts <- q2_starts %>%
  rename(
    date = date_start
  )

q2_starts_w <- left_join(q2_starts, weather, by = "date")
```

Aggregation eines DAtensatzes mit Neighborhoods

```{r}
points1 <- points %>%
  select(start_station_name, start_station_latitude.x, start_station_longitude.x, neighborhood)

q2_starts_w <- left_join(q2_starts_w, points1, by = c("start_station_name"="start_station_name",
                           "start_station_latitude.x"="start_station_latitude.x",
                           "start_station_longitude.x"="start_station_longitude.x"))


#Teilweise fehlt der Bezirk für Station 160, manuell ersetzen, da bekannt
q2_starts_w$neighborhood[q2_starts_w$start_station_name=="E 37 St & Lexington Ave" & is.na(q2_starts_w$neighborhood)] <- "Murray Hill"

#Für Station 3240 kein Bezirk bekannt, fehlende Werte explizit machen
q2_starts_w$neighborhood <- fct_explicit_na(q2_starts_w$neighborhood)
```


Als nächstes füge ich die Wochentag Variable ein und erstelle ein Holidy Feature.

```{r}

#Variable für Wochentag einfügen
q2_starts_w <- q2_starts_w %>%
  mutate(weekday = wday(date, abbr = F, label = T, week_start = 1))
q2_starts_w$weekday <- factor(q2_starts_w$weekday, ordered = FALSE)

#Variable für Feiertag einfügen
q2_starts_w$holiday <- ifelse(isBizday(as.timeDate(q2_starts_w$date), holidayNYSE(2016))==FALSE,1,0)
```


Prüfen auf fehlende Werte.

```{r}
apply(q2_starts_w, 2, function(x) sum(is.na(x)))
```


Nicht brauchbare Spalten entfernen.

```{r}



q2_starts_w$male <- NULL
q2_starts_w$woman <- NULL
q2_starts_w$undefined <- NULL
q2_starts_w$start_station_latitude.x <- NULL
q2_starts_w$start_station_longitude.x <- NULL

q2_starts_w2 <- q2_starts_w

q2_starts_w$neighborhood <- NULL 

q2_starts_w2_agg <- q2_starts_w2 %>%
  group_by(date, neighborhood) %>%
  summarise( anzahl = n(), avg_tripduration = mean(avg_tripduration), avg_age=mean(avr_age))



q2_starts_w2_agg <- left_join(q2_starts_w2_agg, weather, by = "date")


#Variable für Wochentag einfügen
q2_starts_w2_agg <- q2_starts_w2_agg %>%
  mutate(weekday = wday(date, abbr = F, label = T, week_start = 1))
q2_starts_w2_agg$weekday <- factor(q2_starts_w2_agg$weekday, ordered = FALSE)


#Variable für Feiertag einfügen
q2_starts_w2_agg$holiday <- ifelse(isBizday(as.timeDate(q2_starts_w2_agg$date), holidayNYSE(2016))==FALSE,1,0)#




```



```{r}
#Regression mit q2_starts_w
options(max.print=450)
model_q2 <- as.formula(anzahl ~ .)
lin_reg_q2 <- lm(model_q2, q2_starts_w)
summary(lin_reg_q2)
```


```{r}
#Regression mit q2_starts_w2_agg
options(max.print=450)
model_q2_w2_agg <- as.formula(anzahl ~ .)
lin_reg_q2_w2_agg <- lm(model_q2_w2_agg, q2_starts_w2_agg)
summary(lin_reg_q2_w2_agg)

```


### Test Data

```{r, eval=FALSE, echo=TRUE}
feb2017 <- read_csv("Data/201702-citibike-tripdata.csv")
may2017 <- read_csv("Data/201705-citibike-tripdata.csv")
aug2017 <- read_csv("Data/201708-citibike-tripdata.csv")
nov2017 <- read_csv("Data/201711-citibike-tripdata.csv")


saveRDS(feb2017, "Data/feb2017.rds")
saveRDS(may2017, "Data/may2017.rds")
saveRDS(aug2017, "Data/aug2017.rds")
saveRDS(nov2017, "Data/nov2017.rds")
```


```{r}
test_feb2017 <- read_rds("Data/feb2017.rds")
test_may2017 <- read_rds("Data/may2017.rds")
test_aug2017 <- read_rds("Data/aug2017.rds")
test_nov2017 <- read_rds("Data/nov2017.rds")

names(test_feb2017) <- str_replace_all(names(test_feb2017), c(" " = "_"))
names(test_may2017) <- str_replace_all(names(test_may2017), c(" " = "_"))
names(test_aug2017) <- str_replace_all(names(test_aug2017), c(" " = "_"))
names(test_nov2017) <- str_replace_all(names(test_nov2017), c(" " = "_"))


test_feb2017 <- test_feb2017 %>%
  rename(tripduration = Trip_Duration,
         starttime = Start_Time,
         stoptime = Stop_Time,
         start_station_id = Start_Station_ID,
         start_station_name = Start_Station_Name,
         start_station_latitude = Start_Station_Latitude,
         start_station_longitude = Start_Station_Longitude,
         end_station_id = End_Station_ID,
         end_station_name = End_Station_Name,
         end_station_latitude = End_Station_Latitude,
         end_station_longitude = End_Station_Longitude,
         bikeid = Bike_ID,
         usertype = User_Type,
         birth_year = Birth_Year,
         gender = Gender)


apply(test_feb2017, 2, function(x) sum(is.na(x)))
apply(test_may2017, 2, function(x) sum(is.na(x)))
apply(test_aug2017, 2, function(x) sum(is.na(x)))
apply(test_nov2017, 2, function(x) sum(is.na(x)))

```



### Lineare Regression

Um eine Vergleichsmöglichkeit für die Vorhersage der KNNs zu schaffen, wird eine lineare Regression als Benchmark durchgeführt. Es werden für die beiden Forschungsfragen lineare Regressionsmodelle erstellt, die mit den Testdaten eine Vorhersage für die Nutzeranzahl treffen. Der Fehler, der hierbei gemacht wird wird in Form des **M**ean **S**quared **E**rror (MSE) gemessen und mit dem des KNN verglichen.

#### Stündliche Regression

Für die stündlichen Nutzerdaten werden zwei Regressionen durchgeführt. Dies ist der Fall, da sich die Granularität der beiden Datensätze voneinander unterscheidet. Für einen Datensatz sollen die Nutzerzahlen je Bezirk bestimmt werden, für den anderen Datensatz die Nutzerzahlen je Station.

```{r}
#Regression mit Bezirken
model_q1_nh <- as.formula(user_count ~ .)
lin_reg_nh <- lm(model_q1_nh, hourly_starts_nh)
summary(lin_reg_nh)

#Bestimmen welche Variablen signifikant sind
test_sig <- summary(lin_reg_nh)$coefficients[-1,4] < 0.05
#Nur signifikante Behalten
sig_x <- names(test_sig)[test_sig == TRUE] 
View(sig_x)
sig_x <- str_replace_all(sig_x, c(" " = "_"))
sig_x <- str_replace_all(sig_x, c("-" = "_"))
#Model mit nur signifikanten Variablen erstellen FEHLER unerwartetes symbol
sig.model <- as.formula(paste("user_count", paste(sig_x, collapse = " + "), sep = "~"))


```

```{r}
#Regression mit Stations: zu viele Variablen? Kann mit 12,7 GB vektor nicht allozieren
model_q1 <- as.formula(user_count ~ .)
lin_reg_q1 <- lm(model_q1, hourly_starts)
```

### KNN

#### Stündliche Daten Aufbereiten

Zunächst wird der Datensatz auf Basis der Bezirke betrachtet.
Um ein künstlihes neuornales Netz zu erhalten, das fehlerfrei läuft, muss der Datensatz der stündlichen Fahrten noch weiter aufbereitet werden. Für die Spalten neighborhood und weekday werden Dummyvariablen erstellt. Dies ist notwendig, da es sich um kategoriale Variablen handelt. Da eine reine Quantifizerung der Variablen durch hohe Zahlenwerte für einige Kategorein eine Verzerrung der Analysen zur Folge hätte, werden binäre Variblen für jede Kategorie erstellt, die die Werte 0 und 1 annehmen. Um bei der späteren Anwendung der künstlichen neuronalen Netze keine Fehler zu produzieren, werden die Dummys effektkodiert. Statt des Wertes 0 wird der Wert -1 genutzt.

```{r}
#Dummyvariablen erstellen
hourly_starts_nh <- dummy_columns(hourly_starts_nh, c("neighborhood", "weekday"), remove_first_dummy = T, remove_selected_columns = T)

#Namen der Dummys bearbeiten
names(hourly_starts_nh) <- str_replace_all(names(hourly_starts_nh), c(" " = "_"))

#Dummys effektkodieren
hourly_starts_nh[,c(4:5, 9:280)] <- effectcoding(hourly_starts_nh[,c(4:5, 9:280)])
```

#### Tägliche Daten Aufbereiten
Vorbereiten der des anderen DAtensatzes

```{r}
#Dummyvariablen erstellen
library(data.table)

q2_starts_w_nn <- as.data.table(q2_starts_w)


q2_starts_w_nn <- dummy_cols(q2_starts_w, c("weekday", "start_station_name"), remove_first_dummy = T, remove_selected_columns = T)

#Namen der Dummys bearbeiten
names(q2_starts_w_nn) <- str_replace_all(names(q2_starts_w_nn), c(" " = "_"))

#Dummys effektkodieren
#q2_starts_w_nn[,c(11:670)] <- effectcoding(q2_starts_w_nn[,c(11:670)])
```

Funktioniert nicht!


Neue Variante mit neighborhoods.

```{r}

#Dummyvariablen erstellen
library(data.table)

q2_starts_w2_nn <- as.data.table(q2_starts_w2_agg)


q2_starts_w2_nn <- dummy_cols(q2_starts_w2_nn, c("weekday", "neighborhood"), remove_first_dummy = T, remove_selected_columns = T)

#Namen der Dummys bearbeiten
names(q2_starts_w2_nn) <- str_replace_all(names(q2_starts_w2_nn), c(" " = "_"))

#Dummys effektkodieren
q2_starts_w2_nn[,c(11:283)] <- effectcoding(q2_starts_w2_nn[,c(11:283)])

```



#### cross validation

## prediction



## Sources
https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016
https://www.citibikenyc.com/system-data
https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data
https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf
https://www.ncdc.noaa.gov/cdo-web/confirmation
https://www.ncdc.noaa.gov/cdo-web/datasets/LCD/stations/WBAN:94728/detail